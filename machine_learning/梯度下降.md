#### 梯度下降

###### 基本概念：

```
梯度下降是在监督学习中，为了优化模型参数，求出损失函数J(θi)(loss function)取得最小值时，对应的参数θ值的一种迭代算法
```



###### 数学公式：

​	对于一般的线行回归，假设函数可表示为：
$$
h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_3 + ... + \theta_nx_n
$$
​	其中n为样本特征数，$\theta_i$(i = 0, 1, 2 ... n) 为模型参数，$x_i$(i = 0, 1, 2 ... n)  为每个样本的n个特征值，将上式化简后可得：	
$$
h_\theta(x) = \sum_{i=0}^n\theta_ix_i = \theta^Tx
$$
​	对于上面的假设函数，其损失函数 （平方损失函数）可表示为：
$$
J(\theta_1,\theta_2...,\theta_n) = \frac{1}{2m} \sum_{j=0}^m(h_\theta(x_0^j,x_1^j...x_n^j)-y^j)^2
$$
其中m表示为样本个数。



由此，梯度下降公式可表示为：
$$
\theta_i=\theta_i-\alpha\frac{\partial}{\partial\theta_i}J(\theta_1,\theta_2...,\theta_n)
$$
将损失函数带入上式并计算可得：
$$
\theta_i=\theta_i-\alpha\frac{1}{m} \sum_{j=0}^m(h_\theta(x_0^j,x_1^j...x_n^j)-y^j)x_i^j
$$

###### 公式详解：

下面我们以一个最简单的一元线性回归的假设函数举例，其表达式为：$h_\theta(x) = \theta_0x_0+\theta_1x_1$，我们规定$x_0= 1$



但是我们在推导公式前需要理解以下几个概念和问题，以助于我们更好的理解并推导公式

​	什么是假设函数：

​		假设函数是对现有训练集的线性拟合，以寻求参数的最佳估计值，这就是我们的学习模型，决定了算法的是否能够精准的预测，比如房价预测，给定了一定数量，一定面积的房屋价格作为训练集，当我们任意给出一套房子的面积时，需要准确得预测出房屋的价格。

​	什么是损失函数：

​		这是一个用于衡量我们的监督学习模型好坏的标准之一，我们将关于某个样本的预测值（将参数x代入假设函数后的结果）与实际值（样本给予的y关于x的值）的差值称为损失，损失越小，模型越好，那么用于计算这个损失的函数就叫做损失函数



```
由损失函数可知：

	当损失函数取得最小值时，那么其对应的参数θ的值，则是模型的最优参数，也就是说，此时以θ为参数的学习模型是最忧的，能达到最精准的预测结果
```



​	

​	那么问题就转变成：如何在损失函数取得最小值时，求出其参数$\theta$的值，在高等数学中，我们了解到，只需要对函数求导，令它等于零，其解析解为函数取到最值时对应参数的值。但并不是所有函数都可以求出解析解，此时，就衍生出了梯度下降的方法，通过不断迭代改变参数的值进行试探，直至其收敛，就得到了我们需要的参数的解，



```markdown
那么问题又来了，梯度下降是如何工作的呢？为什么在迭代时要减去损失函数的偏导呢？为什么不是其他值呢？
```



```markdown
我们都知道，偏导数的几何意义是多元函数沿坐标轴的变化率，也就是斜率。这里我们引入一个例子，想象一下我们在一座山上，我们目的是下山，那么自然而然的我们想到在走每一步之前需要确定走的方向，为了更快的下山，我们最好的选泽肯定是选择一条最陡峭的（意味着花费时间少，作为距离并不考虑实际安全性）的路，"最陡峭" 映射到我们的坐标系中可以表示为曲面沿着任意方向变化率最大的方向，此时引入了一个新的概念叫做"方向导数"，它可以用来表示曲面沿任意方向的斜率，有了它我们就可以求出"最陡峭"的方向，也就是最大斜率，现在我们的主角"梯度"终于要登场了，梯度就是这个"最陡峭"的方向，它的数学表示为一个由方向导数组成的向量，而它在我们的算法中，梯度就是一个包含了模型参数θ的向量。

在梯度下降算法中，我们不断的寻找合适的方向下山（寻找合适的参数）以至于我们能到达山底（梯度下降的距离小于d（趋近于零）
```



​	我们都知道，偏导数的几何意义是多元函数沿坐标轴的变化率，也就是斜率。这里我们引入一个例子，想象一下我们在一座山上，我们目的是下山，那么自然而然的我们想到在走每一步之前需要确定走的方向。

山坡图如下：

![image-20200427192642191](C:\Users\ZHANGYANJUN0810\AppData\Roaming\Typora\typora-user-images\image-20200427192642191.png)



​	为了更快的下山，我们最好的选泽肯定是选择一条最陡峭的（意味着花费时间少，作为距离并不考虑实际安全性）的路。"最陡峭" 映射到我们的坐标系中可以表示为曲面沿着任意方向变化率最大的方向，此时引入了一个新的概念叫做"方向导数"，它可以用来表示曲面沿任意方向的斜率，有了它我们就可以求出"最陡峭"的方向，也就是最大斜率。



​	假设山坡表示为z = f(x,y), y方向的斜率可由对y的偏微分得到 $f_y(x,y)sin\theta$，x方向的斜率也可以对x偏微分得到$f_x(x,y)cos\theta$.

现在我们有这么一个需求，需要求出u方向的斜率，单位向量$u = cosθi + sinθj$, 我们把这个斜率记作$D_uf$,其表达式为：	
$$
D_uf(x,y) = f_x(x,y)cos\theta + f_y(x,y)sin\theta
$$


现在我们的主角"梯度"终于要登场了，梯度就是这个"最陡峭"的方向，它的数学含义表示为一个由方向导数的最大值组成的向量，而它在我们的算法中，梯度就是一个包含了模型参数θ的向量。



设：$A = (f_x(x,y), f_y(x,y))$，$I = (cos\theta, sin\theta)$，那么我们可以得到$D_Uf(x,y) = A·I = |A| *|I|*cos\theta$，我们令梯度为$J(x,y)$，那么$J(x,y) = maxD_Uf(x,y) =|A| *|I|*cos0 = A = (f_x(x,y), f_y(x,y))$，其中$\theta$为0度



在梯度下降算法中，我们不断的寻找合适的方向下山（寻找合适的参数）以至于我们能到达山底（梯度下降的距离小于d（趋近于零））



现在我们可以回答上面的几个问题了:

梯度下降就是在通过迭代的方式寻找一个合适的参数$\theta$使得损失函数值无限趋近于零（收敛），也就是损失函数取得最小值，而减去损失函数的偏导的意义就在于，为了加快迭代速度（例子中的下山速度），能够更快的得到我们需要的参数$\theta$

在算法执行过程中，对于损失函数值无限趋近于零，我们也可以理解成梯度中的每一个数的绝对值都不大于一个数（这个数是人为设定的，一般是$1e^-5$，这将决定你的算法什么时候终止

现在，我们可以简单写出一元梯度下降的迭代公式：

​												$\theta_0=\theta_0-\alpha(h_\theta(x_0)-y)x_0$

​												$\theta_1=\theta_1-\alpha(h_\theta(x_1)-y)x_1$

那么现在，我们来分析一下多元梯度下降的迭代公式，已知多元线性回归的假设函数为：
$$
h_\theta(x) = \sum_{i=0}^n\theta_ix_i = \theta^Tx
$$


由正规方程（normal equation)得：      $\theta = (x^Tx)^-1x^Ty$

参考文献：

1. 梯度和方向导数的意义：https://www.zhihu.com/question/36301367

2. 梯度下降算法执行过程：https://blog.csdn.net/weixin_34266504/article/details/94540839